{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Python with Excel (and csv) files\n",
    "\n",
    "Our objective is to present some practical ways Python can be used to automate working with Excel files. Some tools for doing this include:\n",
    "\n",
    "* pandas - [read_excel](https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html), [ExcelWriter](https://pandas.pydata.org/docs/reference/api/pandas.ExcelWriter.html), [to_excel](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_excel.html)\n",
    "* [openpyxl](https://openpyxl.readthedocs.io/en/stable/) - can read, write, and modify Excel files\n",
    "* [XlsxWriter](https://xlsxwriter.readthedocs.io/) - powerful but can't edit existing Excel files https://xlsxwriter.readthedocs.io/working_with_tables.html\n",
    "* [xlwings](https://www.xlwings.org/) - provides \"glue\" between Python and Excel\n",
    "\n",
    "**Note:** It appears that [xlrd](https://pypi.org/project/xlrd/) and [xlwt](https://pypi.org/project/xlwt/) are not longer being maintained (they're only for `xls` files) though they are still in Anaconda. Use openpyxl instead.\n",
    "\n",
    "Along the way, we are going to learn about and use the Python built in library, [pathlib](https://docs.python.org/3/library/pathlib.html), for working with file and folder paths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good resources (many more scattered throughout)\n",
    "\n",
    "* https://pbpython.com/\n",
    "* https://www.datacamp.com/community/tutorials/python-excel-tutorial\n",
    "* Excel Hell presentation: https://github.com/chris1610/pbpython/blob/master/presentations/Escaping-Excel-Hell-with-Python-and-Pandas.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prelude - Review of reading json, csv and Excel files with Python\n",
    "Back in the pcda class, I included some optional material on reading json, csv, and Excel files using Python. In those short lessons, I also included some good info on using PyCharm effectively and basic package management with conda and pip. Before launching into new material, I encourage you to take a quick look at these for a refresher. For your convenience, I've included the notebooks and Python scripts in this module's Downloads file. You can find the screencasts at:\n",
    "\n",
    "* http://www.sba.oakland.edu/faculty/isken/courses/mis5470_w21/python_intro_2.html#optional-advanced-activities\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining and splitting of data in csv and Excel files\n",
    "We will explore a few examples motivated by common scenarios such as:\n",
    "\n",
    "* You have a whole folder full of csv (or Excel) files with the same file structure and you need to combine them into a single file. You might also need to make some changes to the consolidated file.\n",
    "* You have an Excel file with multiple sheets of similarly structured data and you want to consolidate them into a single sheet.\n",
    "* You have an Excel file with data in \"wide\" format and you need to convert it to long format, and then perhaps export out individual files (one per the key column(s) in the long formatted data).\n",
    "* You have an Excel file acting as a simple flatfile database. Periodically, you get new Excel files that need to get appended to the \"database\" file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1 - Concatenating csv files\n",
    "\n",
    "As part of a [research project on hospital capacity planning](http://hselab.org/comparing-predictive-models-for-obstetrical-unit-occupancy-using-caret-part-1.html), I ended up\n",
    "running many computer simulations. For example, there were 150 different\n",
    "scenarios, each requiring 25 runs (replications) of the simulation model. Each\n",
    "run generated a log file that was subsequently processed by a Python script. Much\n",
    "of this post-processing took place in parallel threads with a subset of the\n",
    "scenarios allocated to each thread. After all that, I ended up with a\n",
    "subdirectory containing a number of csv files with identical structure that\n",
    "needed to be combined into one big csv file. Here's a little bit of one of\n",
    "the files.\n",
    "\n",
    "```\n",
    "  scenario,rep,timestamp,num_days,num_visits_obs,num_visits_ldr,num_visits_pp, ...\n",
    "  30,1,2016-03-04 12:44:29.610563,105553,42150.0,42154.0,42159.0,8409.0, ...\n",
    "  30,10,2016-03-04 12:48:00.519651,105553,42288.0,42296.0,42305.0,8500.0, ...\n",
    "  30,11,2016-03-04 12:51:30.571655,105553,42210.0,42209.0,42227.0,8387.0, ...\n",
    "  30,12,2016-03-04 12:55:08.550450,105553,42420.0,42433.0,42457.0,8341.0, ...\n",
    "  30,13,2016-03-04 12:58:48.635709,105553,42277.0,42289.0,42301.0,8429.0, ...\n",
    "  30,14,2016-03-04 13:02:21.325709,105553,42127.0,42135.0,42150.0,8581.0, ...\n",
    "  ... many more rows and columns\n",
    "```\n",
    "\n",
    "I also wanted to make sure that the final combined file was sorted in\n",
    "ascending order by scenario by replication. Even though I only had a handful\n",
    "of files, manually doing the concatenation using Excel and copying and pasting\n",
    "got old quickly. Also, I often face this same problem but have hundreds of csv\n",
    "files to concatenate. Seems like a good excuse to put together a Python script\n",
    "to address this problem.\n",
    "\n",
    "Here is a directory listing of the files to be concatenated.\n",
    "\n",
    "\n",
    "```\n",
    " Directory of C:\\Users\\isken\\Documents\\teaching\\aap_s21\\excel_with_python\\data\\sim_output\n",
    "\n",
    "03/31/2021  06:56 AM    <DIR>          .\n",
    "03/31/2021  06:56 AM    <DIR>          ..\n",
    "02/26/2016  01:15 PM           260,332 results_Exp9_Tandem05_nodischadj_0.csv\n",
    "02/27/2016  06:49 AM         1,466,932 results_Exp9_Tandem05_nodischadj_100.csv\n",
    "02/26/2016  07:18 PM           585,564 results_Exp9_Tandem05_nodischadj_1_2.csv\n",
    "02/26/2016  11:05 PM           876,058 results_Exp9_Tandem05_nodischadj_3_5.csv\n",
    "02/27/2016  02:16 AM         1,170,356 results_Exp9_Tandem05_nodischadj_6_9.csv\n",
    "               5 File(s)      4,359,242 bytes\n",
    "```\n",
    "As you can tell from the listing, this is on a Windows system. However, we want a solution that\n",
    "works on Windows, Linux or Mac. The backslash vs forward slash issue in file paths can cause grief. When I [first developed a Python solution](http://hselab.org/concat-csv-pandas.html) for this problem (on a Linux system), I did the typical thing and used the built in `os` and `glob` libraries in Python to work with file paths and the file system. Since then, I've learned about the newish system library called `pathlib`. Let's start by checking out these libraries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with paths and folders and the os\n",
    "\n",
    "For tasks like the one above and many similar such tasks, we need a way to work with the file system and file paths. We can use the `os` module for working with paths and doing things like navigating directories. In addition, the `glob` module is handy for getting lists of files based on a filename pattern.\n",
    "\n",
    "An alternative to `os` and `glob` is the object oriented `pathlib` moddule. Let's take a quick look at both of these approaches. This is **NOT** meant to be a comprehensive tutorial on `pathlib`. There's a really good post on it in Practical Business Python entitled [Using Python's pathlib module](https://pbpython.com/pathlib-intro.html). And, a good two part series on `pathlib` was done by Python blogger, Trey Hunter:\n",
    "\n",
    "* [Why you should be using pathlib](https://treyhunner.com/2018/12/why-you-should-be-using-pathlib/)\n",
    "* [No really, pathlib is great](https://treyhunner.com/2019/01/no-really-pathlib-is-great/)\n",
    "\n",
    "Of course, the [official pathlib docs](https://docs.python.org/3/library/pathlib.html) are really useful - there's even a [table of os and pathlib equivalents](https://docs.python.org/3/library/pathlib.html#correspondence-to-tools-in-the-os-module).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using os and glob\n",
    "This is just a glimpse at a few operations with `os`. One thing to note is that the result of things like `getcwd` is a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrieve current working directory (`cwd`)\n",
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "print(type(cwd))\n",
    "\n",
    "# List all files and directories in data directory\n",
    "print(os.listdir(\"./data\"))\n",
    "\n",
    "# Use glob to get all the csv filenames\n",
    "for csv_fn in glob(\"./data/*.csv\"):\n",
    "    print(csv_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using pathlib\n",
    "\n",
    "Now let's get a brief introduciton to `pathlib`. Again, check out the links above for thorough tutorials.\n",
    "\n",
    "First, let's import the `Path` object from `pathlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the current working directory. Notice that the result is not a string; it's a `WindowsPath` object. If we were on Linux, it would be a `PosixPath` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Path.cwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can build up path objects using the `/` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = Path.cwd() / \"data\"\n",
    "data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also build up paths using the `Path` object. Note in this example, we get a relative path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "in_file_1 = Path(\"data\", \"data-text.csv\")\n",
    "in_file_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted the absolute path associated with this relative path, we just use the `absolute` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "in_file_1.absolute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the parts of a path with the `parts` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "in_file_1.parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to build up path objects is with the `joinpath` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "in_file_2 = Path.cwd().joinpath(\"data\").joinpath(\"data-text.csv\")\n",
    "in_file_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do globbing we can use `pathlib`'s `glob` and `rglob` methods. It's important to note that these return generators - i.e. they don't actually compute all the values until asked. The 'r' in `rglob` stands for *recursive* and means it will dive down into subdirectories looking for filenames that match the specified pattern. The `glob` version just looks in the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir.rglob('*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for f in data_dir.rglob('*.csv'):\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat using `glob` and just print the fiename and its component parts instead of the whole path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for f in data_dir.glob('*.csv'):\n",
    "    print(f\"filename = {f.name}, stem = {f.stem}, suffix = {f.suffix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `iterdir` method returns `Path` objects for everything in the directory from which it is used. It's somewhat like `os.listdir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for d in data_dir.iterdir():\n",
    "    print(type(d), d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pathlib` module includes methods for common filesystem operations. For example, let's create a new directory called `csvs` in our current working directory. Then let's find and copy all of the csv files that are in the `data` folder or any subfolders inside of `data`. Unfortunately, there isn't an actual `copy` method in `pathlib` that copies files (including metadata). For that part we need to use the built in `shutil` library. The `pathlib` modules does have a `rename` method that essentially allows you to move files and `write_bytes` and `write_text` which allows you to copy file contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a Path object corresponding to the new directory name\n",
    "csvs_dir = Path.cwd() / 'csvs'\n",
    "print(csvs_dir)\n",
    "# Make the new directory\n",
    "csvs_dir.mkdir(exist_ok=True)  # Since we are just practicing, we'll overwrite the directory if it already exists.\n",
    "# The Path module has several useful is_* functions\n",
    "print(csvs_dir.is_dir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use `rglob` along with `shutil.copy` to actually copy the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for f in data_dir.rglob('*.csv'):\n",
    "    shutil.copy(f, csvs_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to Example 1 - Concatenating csv files\n",
    "\n",
    "Now that we know how to work with `pathlib`, let's sketch out a strategy for combining all of the csv files in the `sim_output` directory into a single consolidated csv file, named `sim_output.csv`. Let's put the consolidated file in the `data` folder. As mentioned earlier, the consolidate file should be sorted by `scenario` and `rep`.\n",
    "\n",
    "* Create `Path` objects for the source and destination of our files\n",
    "* Create an empty dictionary that will store the intermediate `DataFrame` objects as they get created.\n",
    "* Use `pathlib.glob` to loop over all the csv files in the source directory\n",
    "    - Get the filename stem of the current csv file\n",
    "    - read the csv file into a pandas DataFrame\n",
    "    - store the DataFrame in our dictionary using the stem as the key\n",
    "* Use the pandas `concat` function to concatenate the dataframes in the dictionary, creating the consolidated `DataFrame`\n",
    "* Sort the consolidated `DataFrame` by scenario and rep\n",
    "* Export the consolidated `DataFrame` to a csv file in the desired location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Inputs\n",
    "csvs_path = Path.cwd() / 'data' / 'sim_output'\n",
    "dest_path = Path.cwd() / 'data' / 'sim_output.csv'\n",
    "\n",
    "sortkeys = ['scenario', 'rep']\n",
    "\n",
    "# Create empty dict to hold the DataFrames created as we read each csv file\n",
    "dfs = {}\n",
    "\n",
    "# Loop over all the csv files \n",
    "for csv_f in csvs_path.glob('*.csv'):\n",
    "    # Split the filename off from csv extension. We'll use the filename\n",
    "    # (without the extension) as the key in the dfs dict.\n",
    "    fstem = csv_f.stem\n",
    "\n",
    "    # Read the next csv file into a pandas DataFrame and add it to\n",
    "    # the dfs dict.\n",
    "    df = pd.read_csv(csv_f)\n",
    "    dfs[fstem] = df\n",
    "\n",
    "# Use pandas concat method to combine the file specific DataFrames into\n",
    "# one big DataFrame.\n",
    "bigdf = pd.concat(dfs)\n",
    "\n",
    "# Since we didn't try to control the order in which the files were read,\n",
    "# we'll sort the final DataFrame in place by the specified sort keys.\n",
    "bigdf.sort_values(sortkeys, inplace=True)\n",
    "\n",
    "# Export the final DataFrame to a csv file. Suppress the pandas index.\n",
    "bigdf.to_csv(dest_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2 - Consolidating data from multiple sheets in a Excel file using pandas\n",
    "\n",
    "If you reviewed the `reading_excel_data.ipynb` notebook (and the associated `reading_excel_data.py` file) , you saw that we had some stream temperature data on six different sheets. In that notebook and Python script, we used the xlrd library as well as the openpyxl library to consolidate the data into a single csv file. \n",
    "\n",
    "![sites_1_6](images/sites_1_6.png)\n",
    "\n",
    "While libraries like openpyxl are quite powerful and let you really dig into the details of reading and writing Excel files with Python, sometimes you can get by with a simpler approach. In this case, each sheet is structured the same:\n",
    "\n",
    "* header in first row\n",
    "* data follows\n",
    "\n",
    "Pandas includes a quite powerful and flexible `read_excel` function that is more than up to the task. Let's check it out at https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the default `sheet_name=0` which means only the first sheet is read. To read all the sheets, we need `sheets=None` We have a header in the first row and `header=0` is default, so we can take the default. The following should read all the sheets and return an `OrderedDict` whose keys are the sheet names. \n",
    "\n",
    "Also, we see that `read_excel`, while happy to accept a filename, is also happy to accept at `Path` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sites_1_6_xl = Path('data', 'sites_1_6.xlsx')\n",
    "sites_1_6_consolidated_xl = Path('data', 'sites_1_6_consolidated.xlsx')\n",
    "sites_1_6_consolidated_csv = Path('data', 'sites_1_6_consolidated.csv')  # We'll try this in a bit\n",
    "sites_1_6_xl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sites_all = pd.read_excel(sites_1_6_xl, sheet_name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sites_all.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sites_all['Site1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add a new column to each dataframe corresponding to the sheet name and then consolidate all the dataframes into a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sheet_names = sites_all.keys()\n",
    "for sheet_name in sheet_names:\n",
    "    # Add new col\n",
    "    sites_all[sheet_name]['site'] = sheet_name\n",
    "\n",
    "# Ignore the index so that we don't get a multi-index based on the sheet name\n",
    "consolidated_df = pd.concat(sites_all, ignore_index=True)\n",
    "consolidated_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write it back out to an Excel file. Let's not include the index. After running the next command, we'll go look at the resulting Excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "consolidated_df.to_excel(sites_1_6_consolidated_xl, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For another approach we could use the openpyxl library. While not really needed with this nicely formatted Excel file, sometimes we need finer control over what we are reading in from Excel. This simple example shows how to access specific sheets and cells. I've written it as a function that accepts filenames. Hmmm, I wonder if openpyxl is now open to accepting `Path` objects. Let's try it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def consolidate_logger_xlsheets_openpyxl(fn_xlsx, fn_csv, verbose=True):\n",
    "    \"\"\"\n",
    "    Combine xlsx sheets and output as csv using openpyxl\n",
    "\n",
    "    Args:\n",
    "        fn_xlsx: filename of Excel file to process\n",
    "        fn_csv:  filename of output CSV file\n",
    "        verbose: If true (default), writes status messages\n",
    "    Returns:\n",
    "        nothing, just writes CSV file\n",
    "\n",
    "    \"\"\"\n",
    "    # Open the workbook\n",
    "    wb_sites = openpyxl.load_workbook(fn_xlsx)\n",
    "\n",
    "    # Accumulate the rows (lists) into another list. \n",
    "    # We'll end up with a list of lists.\n",
    "    data = []\n",
    "    for ws in wb_sites:\n",
    "        if verbose:\n",
    "            print(\"openpyxl:\", ws.title)\n",
    "\n",
    "        site = ws.title\n",
    "\n",
    "        # Get the rows for the current sheet\n",
    "        #for row in ws.rows[1:]:\n",
    "        for row in ws.iter_rows():\n",
    "            ws_data = [cell.value for cell in row]\n",
    "            ws_data.append(site)\n",
    "            data.append(ws_data)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"openpyxl:\", \"Total number of rows: \", len(data), \"\\n\")\n",
    "\n",
    "\n",
    "    # Write CSV file\n",
    "    with open(fn_csv, \"w\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "consolidate_logger_xlsheets_openpyxl(sites_1_6_xl, sites_1_6_consolidated_csv, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brilliant, it works! It seems like `pathlib` is getting good adoption and just this little bit I've used it has won me over."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with wide data and a multi-row header - CHALLENGE\n",
    "For the next Excel data wrangling task, you will do the work. In the `data` folder you'll find an Excel file named `temp_data_wide.xlsx`. This was actually a real file we received from Antarctica during our stream temperature related research project. Usually we would get the data from each temperature logger in a separate Excel or csv file. But, many times, the scientists collecting the data would do a little Excel data prep on their own and combine the files into a single Excel file with separate columns for each data logger. The example file I've provided looks like this:\n",
    "\n",
    "![temp_data_wide.xlsx](images/temp_data_wide.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our eventual goal is to read this Excel file and create the following outputs. Each should end up in the `output` folder. You can find examples of my solution outputs in that folder.\n",
    "\n",
    "### Excel file in long form\n",
    "\n",
    "We want a new Excel file that has the data in \"long\" form. In other words, it should look like the following. Notice the new column names - your should have these same column names.\n",
    "\n",
    "![temp_data_long](images/temp_data_long.png)\n",
    "\n",
    "**HINT:** `melt`\n",
    "\n",
    "### Excel file containing summary stats\n",
    "\n",
    "We also want an Excel file containing summary stats by `loc_id`. It should look like this:\n",
    "\n",
    "![summary_stats.png](images/summary_stats.png)\n",
    "\n",
    "**HINT:** `describe`\n",
    "\n",
    "### A plot of the data\n",
    "\n",
    "Finally, we want a faceted plot. It should look like this:\n",
    "\n",
    "![temp_data_plot_soln.png](images/temp_data_plot_soln.png)\n",
    "\n",
    "**HINT:** Seaborn\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just try to get each piece working in the order presented above. You do **NOT** need tools like openpyxl. A combination of pandas, pathlib, matplotlib, and Seaborn is enough to do this. \n",
    "\n",
    "**MORE ADVANCED** Once you've got all the pieces working and generating the correct outputs, then it's time to turn this into a reusable function. Our function would take the following inputs:\n",
    "\n",
    "* **wide_xlfile** - a path object or Excel filename for the wide formatted data\n",
    "* **long_xlfile** (default=None) - a path object or Excel filename for the long formatted Excel file to create\n",
    "* **stats_xlfile** (default=None) - a path object or Excel filename for the summary stats Excel file to create\n",
    "* **plot** (default=None) - a path object or png filename for the plot to create\n",
    "* **\\*\\*read_excel_kwargs** Any other keyword arguments that you want to pass to the pandas read_excel function. \n",
    "\n",
    "This last `**read_excel_kwargs` will give us more flexibility in that we can specify whichever rows we want to skip and/or the location of the header (or any other valid arguments to the `read_excel` function.\n",
    "\n",
    "The **wide_xlfile** argument is required. The other named arguments are optional and default to None. If their value is None, then our function won't output that particular entity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOLUTION - it's at the bottom of the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appending new spreadsheet data in consolidated Excel workbook\n",
    "\n",
    "For our last example, we'll look at appending new data to a simple Excel workbook based \"database\". This is a common scenario in which you've got an ever growing Excel workbook that gets new data appended in it at some, often fixed, frequency (such as daily or monthly). We'll use data from a telephone ACD system. I've simplified the problem for this example by doing the data cleaning necessary to create the \"new data\" spreadsheet from a more complicated text file (if you took my MIS 4460/5460 class you might recall the Muddy Data Tutorial). We'll automate that step some other day; for now let's just focus on appending new data to an existing workbook in which the format of the new data already matches the format of the consolidated workbook (the \"database\"). You'll find both the database workbook and the new data workbook in the `data/acd` folder. You'll also find a backup copy of the database workbook in case we destroy the original with some bad code. :)\n",
    "\n",
    "To pull this off we'll use a combination of pandas and openpyxl. In this example, we aren't going to worry much about data formatting. Both [openpyxl](https://openpyxl.readthedocs.io/en/stable/formatting.html#conditional-formatting) and [XlsxWriter](https://xlsxwriter.readthedocs.io/working_with_conditional_formats.html) are capable of doing all kinds of formatting including Conditional Formatting. One drawback of XlsxWriter is that it cannot read or edit existing Excel files - it's just for writing new ones. That's why I tend to use openpyxl as my general Python tool for working with Excel files.\n",
    "\n",
    "At a high level, we need to be able to append new data to a specific, to be determined, spot in an existing Excel workbook.\n",
    "\n",
    "https://stackoverflow.com/questions/47737220/append-dataframe-to-excel-with-pandas\n",
    "\n",
    "https://stackoverflow.com/questions/38074678/append-existing-excel-sheet-with-new-dataframe-using-python-pandas/38075046#38075046"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The pandas `ExcelWriter` object\n",
    "\n",
    "Pandas has a [specialized `ExcelWriter` object](https://pandas.pydata.org/docs/reference/api/pandas.ExcelWriter.html) that can be used with its `to_excel` function. This object gives us the ability to write data in *append mode* as well as doing some basic date and datetime formatting. According to the docs, it should be used as a context manager - this ensures that the destination file is properly opened and then closed after writing is done. The code below is from the pandas docs - note the `mode=a` for appending.\n",
    "\n",
    "```\n",
    "with ExcelWriter('path_to_file.xlsx', mode='a') as writer:\n",
    "    df.to_excel(writer, sheet_name='Sheet3')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The basic approach\n",
    "\n",
    "We will:\n",
    "\n",
    "* open the workbook with the new data using `openpyxl.load_workbook`\n",
    "* open the database workbook using `ExcelWriter`\n",
    "* find the last row in the database workbook\n",
    "* write the new data in the next available row in the database workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "str(Path('data', 'acd', 'acd_interval_base.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook\n",
    "\n",
    "new_data_xl = Path('data', 'acd', 'acd_interval_20210312.xlsx')\n",
    "database_xl = str(Path('data', 'acd', 'acd_interval_base.xlsx'))\n",
    "\n",
    "dest_sheet_name = 'interval_db'\n",
    "\n",
    "# Read the new data into a pandas dataframe\n",
    "new_data_df = pd.read_excel(new_data_xl, sheet_name=0)\n",
    "\n",
    "# Create the ExcelWriter\n",
    "with pd.ExcelWriter(database_xl, engine='openpyxl', mode='a') as writer:\n",
    "\n",
    "    # Open database workbook\n",
    "    writer.book = load_workbook(database_xl)\n",
    "\n",
    "    # Get last row in database workbook - this is an actual Excel row number\n",
    "    startrow = writer.book[dest_sheet_name].max_row\n",
    "    print(f\"Last row of data: {startrow}\")\n",
    "    \n",
    "    # Since pandas to_excel uses a 0 based index for the rows, we want to start\n",
    "    # writing at startrow\n",
    "    \n",
    "    # Without this next line, to_excel will create a new sheet and put the data there.\n",
    "    # See https://stackoverflow.com/questions/38074678/append-existing-excel-sheet-with-new-dataframe-using-python-pandas/38075046#38075046\n",
    "    # copy existing sheets\n",
    "    writer.sheets = {ws.title:ws for ws in writer.book.worksheets}\n",
    "    \n",
    "    # Now that the writer object is \"aware\" of the sheets it can access, to_excel seems to work fine\n",
    "    new_data_df.to_excel(writer, sheet_name=dest_sheet_name, startrow=startrow, index=False, header=False)\n",
    "    print(f\"New last row of data: {writer.book[dest_sheet_name].max_row}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, this is just the start of something useful. We have no checks to make sure we don't append the same data twice, for example. We shouldn't be using Excel as a database anyway, but people do it. :) The StackOverflow link in the code above has a more complete function for this task in the accepted answer - very nice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More on using Python for similar Excel tasks\n",
    "Chris Moffit, in his Practical Business Python blog ([roadmap to the posts](https://pbpython.com/roadmap.html)), has many terrific posts that show how to use Python for common Excel tasks. If you use Excel and Python, his blog is an absolute **must read**. The posts include:\n",
    "\n",
    "* [Combining Data From Multiple Excel Files](https://pbpython.com/excel-file-combine.html) - file globbing, concatenating dataframes,\n",
    "* [Reading Poorly Structured Excel Files with Pandas](https://pbpython.com/pandas-excel-range.html) - advanced use of `read_excel`, accessing ranges and Tables\n",
    "* [Common Excel Tasks Demonstrated in Pandas](https://pbpython.com/excel-pandas-comp.html) - totals rows, fuzzy string matching\n",
    "* [Common Excel Tasks Demonstrated in Pandas - Part 2](https://pbpython.com/excel-pandas-comp-2.html) - selection and filtering\n",
    "* [Improving pandas Excel output](https://pbpython.com/improve-pandas-excel-output.html) - using XlsxWriter to format Excel workbooks from Python\n",
    "* [Creating Advanced Excel Workbooks](https://pbpython.com/advanced-excel-workbooks.html) - XlsxWriter, inserting VBA from Python(!), using COM to merge sheets\n",
    "* [Interactive Data Analysis with Python and Excel ](https://pbpython.com/xlwings-pandas-excel.html) - using xlwings to \"glue\" Python and Excel together, using sqlalchemy to interact with databases\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with wide data and a multi-row header - SOLUTION\n",
    "\n",
    "First I got it working in steps. Then I turned it into a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create Path objecst for Excel file to process, summary stats and plot to output\n",
    "temp_data_wide_xl = Path('data', 'temp_data_wide.xlsx')\n",
    "temp_data_long_xl = Path('output', 'temp_data_long_soln.xlsx')\n",
    "temp_data_stats_xl = Path('output', 'temp_data_stats_soln.xlsx')\n",
    "temp_data_plot_png = Path('output', 'temp_data_plot_soln.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read into pandas with read_excel. Need to skip rows 1 and 2.\n",
    "temp_data_wide_df = pd.read_excel(temp_data_wide_xl, header=0, skiprows=[1,2])\n",
    "temp_data_wide_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Rename the Label col to datetime\n",
    "temp_data_wide_df.rename({'Label': 'datetime'}, axis=1, inplace=True)\n",
    "temp_data_wide_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reshape the data from wide to long\n",
    "temp_data_long_df = temp_data_wide_df.melt(id_vars=['datetime'], var_name='loc_id', value_name='temp_c')\n",
    "temp_data_long_df.to_excel(temp_data_long_xl, index=None)\n",
    "temp_data_long_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute the summary stats\n",
    "summary_stats = temp_data_long_df.groupby(['loc_id'])['temp_c'].describe()\n",
    "summary_stats.to_excel(temp_data_stats_xl)\n",
    "summary_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally create the faceted plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "temp_g = sns.FacetGrid(temp_data_long_df, row=\"loc_id\", sharey=True, height=2.5, aspect=11.7/2.5)\n",
    "temp_g = temp_g.map(plt.plot, \"datetime\", \"temp_c\")\n",
    "temp_g.savefig(temp_data_plot_png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **wide_xlfile** - a path object or Excel filename for the wide formatted data\n",
    "* **long_xlfile** (default=None) - a path object or Excel filename for the long formatted Excel file to create\n",
    "* **stats_xlfile** (default=None) - a path object or Excel filename for the summary stats Excel file to create\n",
    "* **plot** (default=None) - a path object or png filename for the plot to create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_wide_temp_file(wide_xlfile, long_xlfile=None, stats_xlfile=None, plot=None, **read_excel_kwargs):\n",
    "    \"\"\"Process wide formatted stream temperature data file\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    wide_xlfile : Path object\n",
    "        The wide formatted Excel data file\n",
    "    long_xlfile : optional (default is None), Path object\n",
    "        Long formatted Excel data file created if not None\n",
    "    stats_xlfile : optional (default is None), Path object\n",
    "        Excel file for summary stats created if not None\n",
    "    plot : optional (default is None), Path object\n",
    "        Time series temperature plot created if not None\n",
    "    **read_excel_kwargs : keyword and values (can be dict)\n",
    "        arguments which will be passed to `DataFrame.to_excel()`\n",
    "                            \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    No return value\n",
    "        Generates output files as detailed above\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read into pandas with read_excel. \n",
    "    temp_data_wide_df = pd.read_excel(wide_xlfile, **read_excel_kwargs)\n",
    "    \n",
    "    # Rename the Label col to datetime\n",
    "    temp_data_wide_df.rename({'Label': 'datetime'}, axis=1, inplace=True)\n",
    "    \n",
    "    # Reshape the data from wide to long\n",
    "    temp_data_long_df = temp_data_wide_df.melt(id_vars=['datetime'], var_name='loc_id', value_name='temp_c')\n",
    "    \n",
    "    if long_xlfile is not None:\n",
    "        temp_data_long_df.to_excel(long_xlfile, index=None)\n",
    "    \n",
    "    # Compute the summary stats\n",
    "    if stats_xlfile is not None:\n",
    "        summary_stats = temp_data_long_df.groupby(['loc_id'])['temp_c'].describe()\n",
    "        summary_stats.to_excel(stats_xlfile)\n",
    "        \n",
    "    # Make the plot\n",
    "    if plot is not None:\n",
    "        temp_g = sns.FacetGrid(temp_data_long_df, row=\"loc_id\", sharey=True, height=2.5, aspect=11.7/2.5)\n",
    "        temp_g = temp_g.map(plt.plot, \"datetime\", \"temp_c\")\n",
    "        temp_g.savefig(plot)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "temp_data_wide_xl = Path('data', 'temp_data_wide.xlsx')\n",
    "temp_data_long_xl = Path('output', 'temp_data_long_test.xlsx')\n",
    "temp_data_stats_xl = Path('output', 'temp_data_stats_test.xlsx')\n",
    "temp_data_plot_png = Path('output', 'temp_data_plot_test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "process_wide_temp_file(temp_data_wide_xl, long_xlfile=temp_data_long_xl, stats_xlfile=temp_data_stats_xl, plot=temp_data_plot_png, header=0, skiprows=[1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
